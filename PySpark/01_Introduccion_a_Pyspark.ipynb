{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 1\n",
    "## Configuración de Spark \n",
    "\n",
    "Vamos a trabajar con Python y Spark utilizando PySpark, que es la API de Spark para Python. \n",
    "\n",
    "Esto nos permitirá manejar grandes volúmenes de datos de manera eficiente y realizar análisis complejos. \n",
    "\n",
    "Vamos a configurar nuestro entorno y ejecutar algunos comandos básicos para entender cómo funciona.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalamos pyspark\n",
    "!pip install pyspark\n",
    "\n",
    "# Instalamos findspark\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciando el contexto de Spark\n",
    "import pyspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos PySpark para inicializar el contexto spark.  \n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contexto Spark y Sesión Spark\n",
    "\n",
    "En este ejercicio, vamos a:\n",
    "- Crear el Spark Context\n",
    "- Iniciar la sesión Spark \n",
    "\n",
    "#### Primero empezamos con la creación del Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto no es necesario ejecutarlo\n",
    "# pip install --upgrade pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/11 22:33:11 WARN Utils: Your hostname, eurecat-vm resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/06/11 22:33:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/11 22:33:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Crear una configuración de Spark\n",
    "conf = SparkConf().setAppName(\"Ejemplo Básico de DataFrames con Spark\").setMaster(\"local\")\n",
    "\n",
    "# Crear un Spark Context\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Crear una Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Ejemplo Básico de DataFrames con Spark\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En este caso tenemos dos posibles problemas\n",
    "#### Problema: JAVA_HOME is not set\n",
    "\n",
    "Si tienen este problema: **JAVA_HOME is not set** es necesario abrir la terminal e instalar java\n",
    "Para esto podemos ejecutar en la terminal:\n",
    "\n",
    "```\n",
    "java --version\n",
    "\n",
    "sudo apt install openjdk-8-jre-headless\n",
    "\n",
    "pass: eurecat\n",
    "\n",
    "ingresar: s\n",
    "```\n",
    "\n",
    "Ahora si volvemos a **Visual Studio Code**\n",
    "\n",
    "\n",
    "\n",
    "#### Problema: ValueError: Cannot run multiple SparkContexts at once\n",
    "Problema: **ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at C:\\Users\\lucia.alvarez\\AppData\\Local\\Temp\\ipykernel_13740\\2225446927.py:5**\n",
    "\n",
    "Debemos reutilizar el SparkContext existente\n",
    "\n",
    "Comprobar si hay un SparkContext existente y reutilizarlo\n",
    "\n",
    "```\n",
    "if 'sc' in locals():\n",
    "    sc.stop()\n",
    "```\n",
    "\n",
    "Crear una configuración de Spark\n",
    "\n",
    "```\n",
    "conf = SparkConf().setAppName(\"Ejemplo Básico de DataFrames con Spark\").setMaster(\"local\")\n",
    "```\n",
    "\n",
    "Crear un Spark Context\n",
    "\n",
    "```\n",
    "sc = SparkContext(conf=conf)\n",
    "```\n",
    "\n",
    "Crear una Spark Session\n",
    "```\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Ejemplo Básico de DataFrames con Spark\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ahora vamos a verificar la Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession está activa y lista para usar.\n"
     ]
    }
   ],
   "source": [
    "# Verificar si la Spark Session está activa\n",
    "if 'spark' in locals() and isinstance(spark, SparkSession):\n",
    "    print(\"SparkSession está activa y lista para usar.\")\n",
    "else:\n",
    "    print(\"SparkSession no está activa. Por favor, crea una SparkSession.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de RDD y Transformaciones\n",
    "En este ejercicio trabajamos con Resilient Distributed Datasets (RDDs). Los RDDs son la abstracción primitiva de datos de Spark y utilizamos conceptos de la programación funcional para crear y manipular RDDs.\n",
    "\n",
    "#### 01. Crear un RDD\n",
    "Creamos un RDD que tiene enteros del 10 al 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primer elemento: 10\n",
      "Longitud del conjunto de datos: 31\n"
     ]
    }
   ],
   "source": [
    "data = range(10, 41)\n",
    "\n",
    "# Imprime el primer elemento del iterador\n",
    "print(\"Primer elemento:\", data[0])\n",
    "print(\"Longitud del conjunto de datos:\", len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un RDD a partir de los datos\n",
    "dataRDD = sc.parallelize(data, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02. Transformaciones en el RDD\n",
    "\n",
    "Una transformación es una operación sobre un RDD que da como resultado un nuevo RDD. \n",
    "\n",
    "El RDD contendrá una serie de transformaciones, o instrucciones de cálculo, que sólo se llevarán a cabo cuando se llame a una acción. En esta transformación, reducimos cada elemento del RDD en 5. \n",
    "\n",
    "Además, filtramos el RDD para que sólo contenga elementos menores a 25.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducimos cada elemento del RDD en 5\n",
    "subRDD = dataRDD.map(lambda x: x - 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos el RDD para que solo contenga elementos menores a 25\n",
    "filteredRDD = subRDD.filter(lambda x: x < 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03: Acciones en nuestro RDD\n",
    "Aplicamos acciones a nuestro RDD transformado para obtener y contar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementos del RDD filtrado: [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Acciones en el RDD\n",
    "print(\"Elementos del RDD filtrado:\", filteredRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de elementos en el RDD filtrado: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de elementos en el RDD filtrado:\", filteredRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 04: Almacenamiento de Datos en Caché\n",
    "Almacenar datos en caché en Spark mejora el rendimiento de las operaciones repetitivas en un RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo para el primer conteo (sin caché): 1.0601565837860107\n",
      "Tiempo para el segundo conteo (con caché): 0.4849233627319336\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "# Crear un nuevo RDD con un rango de datos\n",
    "testRDD = sc.parallelize(range(1000, 51000), 4)\n",
    "\n",
    "\n",
    "# Almacenar el RDD en caché\n",
    "testRDD.cache()\n",
    "\n",
    "# Primera evaluación: esto desencadenará la evaluación y el almacenamiento en caché\n",
    "t1 = time.time()\n",
    "count1 = testRDD.count()\n",
    "dt1 = time.time() - t1\n",
    "print(\"Tiempo para el primer conteo (sin caché):\", dt1)\n",
    "\n",
    "\n",
    "# Segunda evaluación: ahora los datos están cacheados\n",
    "t2 = time.time()\n",
    "count2 = testRDD.count()\n",
    "dt2 = time.time() - t2\n",
    "print(\"Tiempo para el segundo conteo (con caché):\", dt2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finalización de la Sesión de Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detener la sesión de spark\n",
    "spark.stop() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fin de este práctico"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
